{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c488dcc3-980b-4153-9aa7-32da4b8c197b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PATH\"] = \"/opt/homebrew/bin:\" + os.environ.get(\"PATH\", \"\")\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "import zipfile\n",
    "import rarfile\n",
    "import subprocess\n",
    "import gzip\n",
    "import bz2\n",
    "import shutil\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from io import BytesIO\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "from demoparser2 import DemoParser\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from shapely.geometry import Point, Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebb520a-3937-47e8-8d81-86d26fe3a2cb",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6de0a8-8b16-41de-a643-d705e42e43ed",
   "metadata": {},
   "source": [
    "## Webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "793b8496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_undetected_chrome(download_dir: pathlib.Path, headless: bool = False) -> uc.Chrome:\n",
    "    options = uc.ChromeOptions()\n",
    "    prefs = {\n",
    "        \"download.default_directory\": str(download_dir.resolve()),\n",
    "        \"download.prompt_for_download\": False,\n",
    "        \"download.directory_upgrade\": True,\n",
    "        \"safebrowsing.enabled\": True,\n",
    "    }\n",
    "    options.add_experimental_option(\"prefs\", prefs)\n",
    "    options.headless = headless\n",
    "    return uc.Chrome(options=options)\n",
    "\n",
    "def get_soup(driver, url: str, delay: int = 5) -> BeautifulSoup:\n",
    "    driver.get(url)\n",
    "    try:\n",
    "        WebDriverWait(driver, delay).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"a[href^='/matches/']\"))\n",
    "        )\n",
    "    except Exception:\n",
    "        time.sleep(delay)\n",
    "    html = driver.page_source\n",
    "    return BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "def find_match_urls_from_results(driver, max_matches: int = 5, delay: int = 5) -> list[str]:\n",
    "    soup = get_soup(driver, RESULTS_URL, delay = delay)\n",
    "    time.sleep(15)\n",
    "    anchors = soup.select(\"a[href^='/matches/']\")\n",
    "    urls, seen = [], set()\n",
    "    for a in anchors: \n",
    "        href = a.get(\"href\", \"\")\n",
    "        if \"/matches/\" in href:\n",
    "            full = urljoin(HLTV_BASE, href)\n",
    "            if full not in seen: \n",
    "                seen.add(full)\n",
    "                urls.append(full)\n",
    "    print(f\"Found {len(urls)} match links on results page.\")\n",
    "    return urls[:max_matches]\n",
    "    \n",
    "\n",
    "def find_demo_download_url(driver, max_wait=6) -> str:\n",
    "    # match_url: str\n",
    "    # driver.get(match_url)\n",
    "    print(\"Waiting for demo link to become availableâ€¦\")\n",
    "    for i in range(max_wait):\n",
    "        try:\n",
    "            # Try primary robust selector\n",
    "            link = driver.find_element(By.CSS_SELECTOR, \"a.stream-box[data-demo-link-button][data-demo-link]\")\n",
    "            demo_path = link.get_attribute(\"data-demo-link\")\n",
    "            if demo_path:\n",
    "                demo_url = urljoin(HLTV_BASE, demo_path)\n",
    "                print(f\"Demo found via CSS selector: {demo_url}\")\n",
    "                return demo_url\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Fallback: try finding any anchor with /download/demo/ in href\n",
    "        anchors = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        for a in anchors:\n",
    "            href = a.get_attribute(\"href\")\n",
    "            if href and \"/download/demo/\" in href:\n",
    "                print(f\"Demo found via fallback href: {href}\")\n",
    "                return href\n",
    "\n",
    "        time.sleep(10)\n",
    "        print(f\"â€¦ still waiting ({(i+1)*10}s)\")\n",
    "\n",
    "    raise RuntimeError(f\"No demo link found on match page after {max_wait*10}s: {match_url}\")\n",
    "\n",
    "def extract_match_data(driver, match_url: str, delay: int = 5) -> dict:\n",
    "    \"\"\"\n",
    "    Extract detailed match data from a single HLTV match page.\n",
    "    \n",
    "    Returns a dictionary containing:\n",
    "    - match_url: The URL of the match\n",
    "    - team1_name: Name of team 1\n",
    "    - team2_name: Name of team 2\n",
    "    - team1_score: Number of maps won by team 1\n",
    "    - team2_score: Number of maps won by team 2\n",
    "    - map_1 through map_5: Individual map names (None if not played)\n",
    "    - total_maps: Total number of maps played\n",
    "    - date: Match date\n",
    "    - event: Event name\n",
    "    - demo_url: Demo download URL (if available)\n",
    "    \"\"\"\n",
    "    soup = get_soup(driver, match_url, delay=delay)\n",
    "    \n",
    "    match_data = {\n",
    "        'match_url': match_url,\n",
    "        'team1_name': None,\n",
    "        'team2_name': None,\n",
    "        'team1_score': 0,\n",
    "        'team2_score': 0,\n",
    "        'map_1': None,\n",
    "        'map_2': None,\n",
    "        'map_3': None,\n",
    "        'map_4': None,\n",
    "        'map_5': None,\n",
    "        'total_maps': 0,\n",
    "        'date': None,\n",
    "        'event': None,\n",
    "        'demo_url': None\n",
    "    }\n",
    "    \n",
    "    # Extract team names and scores\n",
    "    teams = soup.find_all('div', class_='team')\n",
    "    if len(teams) >= 2:\n",
    "        # Team 1 (left side)\n",
    "        team1 = teams[0]\n",
    "        team1_name_elem = team1.find('div', class_='teamName')\n",
    "        if team1_name_elem:\n",
    "            match_data['team1_name'] = team1_name_elem.text.strip()\n",
    "        \n",
    "        # Check if team1 won or lost and get score\n",
    "        team1_won = team1.find('div', class_='won')\n",
    "        team1_lost = team1.find('div', class_='lost')\n",
    "        if team1_won:\n",
    "            score_text = team1_won.text.strip()\n",
    "            try:\n",
    "                match_data['team1_score'] = int(score_text)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        elif team1_lost:\n",
    "            score_text = team1_lost.text.strip()\n",
    "            try:\n",
    "                match_data['team1_score'] = int(score_text)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        \n",
    "        # Team 2 (right side)\n",
    "        team2 = teams[1]\n",
    "        team2_name_elem = team2.find('div', class_='teamName')\n",
    "        if team2_name_elem:\n",
    "            match_data['team2_name'] = team2_name_elem.text.strip()\n",
    "        \n",
    "        # Check if team2 won or lost and get score\n",
    "        team2_won = team2.find('div', class_='won')\n",
    "        team2_lost = team2.find('div', class_='lost')\n",
    "        if team2_won:\n",
    "            score_text = team2_won.text.strip()\n",
    "            try:\n",
    "                match_data['team2_score'] = int(score_text)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        elif team2_lost:\n",
    "            score_text = team2_lost.text.strip()\n",
    "            try:\n",
    "                match_data['team2_score'] = int(score_text)\n",
    "            except ValueError:\n",
    "                pass\n",
    "    \n",
    "    # Extract maps that were played\n",
    "    maps_played = []\n",
    "    mapholders = soup.find_all('div', class_='mapholder')\n",
    "    for mapholder in mapholders:\n",
    "        # Check if this map was actually played\n",
    "        played = mapholder.find('div', class_='played')\n",
    "        if played:\n",
    "            mapname_elem = mapholder.find('div', class_='mapname')\n",
    "            if mapname_elem:\n",
    "                map_name = mapname_elem.text.strip()\n",
    "                maps_played.append(map_name)\n",
    "    \n",
    "    # Assign maps to individual columns (up to 5 maps)\n",
    "    for i, map_name in enumerate(maps_played[:5], 1):\n",
    "        match_data[f'map_{i}'] = map_name\n",
    "    \n",
    "    match_data['total_maps'] = len(maps_played)\n",
    "    \n",
    "    # Extract date and event\n",
    "    time_and_event = soup.find('div', class_='timeAndEvent')\n",
    "    if time_and_event:\n",
    "        # Extract date\n",
    "        date_elem = time_and_event.find('div', class_='date')\n",
    "        if date_elem:\n",
    "            match_data['date'] = date_elem.text.strip()\n",
    "        \n",
    "        # Extract event name\n",
    "        event_elem = time_and_event.find('div', class_='event text-ellipsis')\n",
    "        if event_elem:\n",
    "            match_data['event'] = event_elem.text.strip()\n",
    "    \n",
    "    # Extract demo download URL\n",
    "    try:\n",
    "        demo_url = find_demo_download_url(driver, max_wait=6)\n",
    "        match_data['demo_url'] = demo_url\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting demo URL: {e}\")\n",
    "        match_data['demo_url'] = None\n",
    "    \n",
    "    return match_data\n",
    "\n",
    "def scrape_multiple_matches(driver, match_urls, delay: int = 5) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Scrape data from multiple matches on the results page.\n",
    "    \n",
    "    Returns a list of dictionaries, each containing match data.\n",
    "    \"\"\"\n",
    "    all_match_data = []\n",
    "    \n",
    "    for i, match_url in enumerate(match_urls, 1):\n",
    "        print(f\"\\nScraping match {i}/{len(match_urls)}: {match_url}\")\n",
    "        try:\n",
    "            match_data = extract_match_data(driver, match_url, delay=delay)\n",
    "            all_match_data.append(match_data)\n",
    "            maps_str = ', '.join([match_data[f'map_{j}'] for j in range(1, 6) if match_data[f'map_{j}'] is not None])\n",
    "            # print(f\"  - {match_data['team1_name']} vs {match_data['team2_name']}: {match_data['team1_score']}-{match_data['team2_score']}\")\n",
    "            # print(f\"  - Maps played: {maps_str}\")\n",
    "            # print(f\"  - Demo URL: {match_data['demo_url']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {match_url}: {e}\")\n",
    "        \n",
    "        # Add delay between requests to avoid rate limiting\n",
    "        if i < len(match_urls):\n",
    "            time.sleep(REQUEST_DELAY_SEC)\n",
    "    \n",
    "    return all_match_data\n",
    "\n",
    "def save_match_data_to_csv(match_data_list: list[dict], output_file: str = \"hltv_matches.csv\"):\n",
    "    \"\"\"\n",
    "    Save scraped match data to a CSV file.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(match_data_list)\n",
    "    \n",
    "    # Reorder columns for better readability\n",
    "    column_order = [\n",
    "        'match_url', 'demo_url', 'date', 'event',\n",
    "        'team1_name', 'team2_name', \n",
    "        'team1_score', 'team2_score',\n",
    "        'total_maps',\n",
    "        'map_1', 'map_2', 'map_3', 'map_4', 'map_5'\n",
    "    ]\n",
    "    \n",
    "    df = df[column_order]\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nData saved to {output_file}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79357a94-9e81-46d0-bd2b-0acd7e43016e",
   "metadata": {},
   "source": [
    "## Downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec894e21-99e0-427a-a378-91dd52e66b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_size(bytes_):\n",
    "    for unit in ['B', 'KB', 'MB', 'GB']:\n",
    "        if bytes_ < 1024:\n",
    "            return f\"{bytes_:3.1f} {unit}\"\n",
    "        bytes_ /= 1024\n",
    "    return f\"{bytes_:3.1f} TB\"\n",
    "\n",
    "def get_demo_number_from_url(demo_url: str) -> str:\n",
    "    # Extract the number at the end of the demo URL, e.g., '102077' from 'https://www.hltv.org/download/demo/102077'\n",
    "    match = re.search(r'/demo/(\\d+)', demo_url)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        raise ValueError(f\"Could not find demo number in URL: {demo_url}\")\n",
    "\n",
    "def download_demo_with_selenium(driver, demo_url: str, out_dir: pathlib.Path) -> pathlib.Path:\n",
    "    driver.get(demo_url)\n",
    "    size_prev = 0\n",
    "    start = time.time()\n",
    "    final_file = None\n",
    "    print(\"â–¶ Starting downloadâ€¦\")\n",
    "    \n",
    "    while True:\n",
    "        candidates = []\n",
    "        for ext in ['zip', 'rar', 'gz', 'bz2', 'dem']:\n",
    "            candidates += list(out_dir.glob(f\"*.{ext}\"))\n",
    "        if candidates:\n",
    "            final_file = max(candidates, key=lambda f: f.stat().st_ctime)\n",
    "            # NEW: Wait for file size to stabilize\n",
    "            time.sleep(3)\n",
    "            stable_size = final_file.stat().st_size\n",
    "            time.sleep(2)\n",
    "            if final_file.stat().st_size == stable_size and stable_size > 0:\n",
    "                print(f\"\\nâœ… Download complete: {final_file.name} ({human_size(stable_size)})\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"\\rðŸ“¦ Finalizing... {human_size(stable_size)}\", end=\"\")\n",
    "                final_file = None\n",
    "                continue\n",
    "\n",
    "        partials = list(out_dir.glob(\"*.crdownload\"))\n",
    "        if partials:\n",
    "            part = partials[0]\n",
    "            size = part.stat().st_size\n",
    "            if size != size_prev:\n",
    "                print(f\"\\rðŸ“¦ Downloading... {human_size(size)}\", end=\"\")\n",
    "                size_prev = size\n",
    "\n",
    "        if time.time() - start > 3600:\n",
    "            print(\"\\nâš  Timed out waiting for download to finish.\")\n",
    "            break\n",
    "        time.sleep(1)\n",
    "    \n",
    "    if not final_file:\n",
    "        raise RuntimeError(\"No finished demo file detected after timeout.\")\n",
    "    \n",
    "    # Additional wait to ensure Chrome finishes all file operations\n",
    "    print(\"Waiting for file system sync...\")\n",
    "    time.sleep(5)\n",
    "    return final_file\n",
    "\n",
    "def extract_dem_from_file(archive_path: pathlib.Path, out_dir: pathlib.Path, name_filter: str = None) -> pathlib.Path:\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    suffix = archive_path.suffix.lower()\n",
    "\n",
    "    def matches_filter(filename):\n",
    "        return name_filter.lower() in filename.lower() if name_filter else True\n",
    "\n",
    "    # Diagnostic: print header bytes\n",
    "    with open(archive_path, 'rb') as f:\n",
    "        header = f.read(16)\n",
    "        print(f\"   Header (hex): {header[:8].hex()}\")\n",
    "\n",
    "    if suffix == \".dem\":\n",
    "        if not matches_filter(archive_path.name):\n",
    "            raise RuntimeError(f\".dem file '{archive_path.name}' does not match filter '{name_filter}'. Skipping.\")\n",
    "        target = out_dir / archive_path.name\n",
    "        shutil.copy2(archive_path, target)\n",
    "        return target\n",
    "\n",
    "    if suffix == \".zip\":\n",
    "        with zipfile.ZipFile(archive_path, \"r\") as z:\n",
    "            dem_members = [m for m in z.namelist() if m.lower().endswith(\".dem\") and matches_filter(m)]\n",
    "            if not dem_members:\n",
    "                raise RuntimeError(f\"No .dem matching filter '{name_filter}' found inside zip: {archive_path.name}\")\n",
    "            # Extract all filtered dem files and return first one\n",
    "            for dem_file in dem_members:\n",
    "                z.extract(dem_file, out_dir)\n",
    "            return out_dir / dem_members[0]\n",
    "\n",
    "    if suffix == \".rar\":\n",
    "        # Find WinRAR/UnRAR executable path\n",
    "        winrar_paths = [\n",
    "            r\"C:\\Program Files\\WinRAR\\WinRAR.exe\",\n",
    "            r\"C:\\Program Files (x86)\\WinRAR\\WinRAR.exe\",\n",
    "            r\"C:\\Program Files\\WinRAR\\UnRAR.exe\",\n",
    "            r\"C:\\Program Files (x86)\\WinRAR\\UnRAR.exe\",\n",
    "        ]\n",
    "        unrar_path = None\n",
    "        for path in winrar_paths:\n",
    "            if os.path.exists(path):\n",
    "                unrar_path = path\n",
    "                break\n",
    "    \n",
    "        if not unrar_path:\n",
    "            raise RuntimeError(\"WinRAR/UnRAR not found! Please install WinRAR or add UnRAR to PATH.\")\n",
    "    \n",
    "        print(f\"[rar] Using: {unrar_path}\")\n",
    "    \n",
    "        # List archive contents using rarfile\n",
    "        with rarfile.RarFile(str(archive_path), 'r') as rf:\n",
    "            file_names = rf.namelist()\n",
    "            matching_files = [f for f in file_names if f.lower().endswith('.dem') and (name_filter is None or name_filter.lower() in f.lower())]\n",
    "    \n",
    "        if not matching_files:\n",
    "            if name_filter:\n",
    "                raise RuntimeError(f\"No .dem files matching filter '{name_filter}' found inside archive.\")\n",
    "            else:\n",
    "                matching_files = [f for f in file_names if f.lower().endswith('.dem')]\n",
    "                if not matching_files:\n",
    "                    raise RuntimeError(\"No .dem files found inside archive.\")\n",
    "    \n",
    "        # Extract only matching files using subprocess with WinRAR/UnRAR\n",
    "        for file_to_extract in matching_files:\n",
    "            print(f\"[rar] Extracting file: {file_to_extract}\")\n",
    "            extract_result = subprocess.run(\n",
    "                [unrar_path, \"x\", \"-y\", str(archive_path), file_to_extract, str(out_dir)],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=600\n",
    "            )\n",
    "            print(f\"[rar] Extract stdout: {extract_result.stdout[:200]}\")\n",
    "            print(f\"[rar] Extract stderr: {extract_result.stderr[:200]}\")\n",
    "    \n",
    "        # After extraction, return first matched .dem file path in out_dir\n",
    "        dem_files = [f for f in out_dir.glob(\"*\") if f.name in matching_files]\n",
    "        if dem_files:\n",
    "            return dem_files[0]\n",
    "        else:\n",
    "            raise RuntimeError(\"Extraction completed but .dem file not found in output.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a7a16f-0e18-4ccb-afcb-470a06b2344f",
   "metadata": {},
   "source": [
    "## Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae9a74ba-c644-43ea-a021-2135112626cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_next_tick_position(df):\n",
    "    # Sort to ensure correctness (by player and tick)\n",
    "    df = df.sort_values(['steamid', 'tick'])\n",
    "    \n",
    "    # Compute next tick positions for each player's ordered ticks\n",
    "    df['X_next'] = df.groupby('steamid')['X'].shift(-1)\n",
    "    df['Y_next'] = df.groupby('steamid')['Y'].shift(-1)\n",
    "    df['tick_next'] = df.groupby('steamid')['tick'].shift(-1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def parse_demo(dem_path: pathlib.Path, base_csv_dir: pathlib.Path, match_name: str, demo_id: str):\n",
    "    dem_name = dem_path.stem\n",
    "\n",
    "    print(f\"Parsing demo: {dem_name}\")\n",
    "    parser = DemoParser(str(dem_path))\n",
    "\n",
    "    KNOWN_MAPS = [\"nuke\", \"mirage\", \"dust2\", \"inferno\", \"overpass\", \"train\", \"vertigo\"]\n",
    "\n",
    "    def extract_map_name(dem_name):\n",
    "        dem_name_lower = dem_name.lower()\n",
    "        for map_name in KNOWN_MAPS:\n",
    "            if map_name in dem_name_lower:\n",
    "                return map_name\n",
    "        match = re.search(r\"\\b(\" + \"|\".join(KNOWN_MAPS) + r\")\\b\", dem_name_lower)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        return None\n",
    "\n",
    "    meta = {\n",
    "        \"match\": match_name,\n",
    "        \"demo_name\": dem_name,\n",
    "        \"demo_path\": str(dem_path),\n",
    "        \"date\": None,\n",
    "        \"map\": extract_map_name(dem_name),\n",
    "        \"total_rounds\": None,\n",
    "        \"rounds_started\": 0,\n",
    "        \"rounds_ended\": 0,}\n",
    "\n",
    "    try:\n",
    "        meta[\"date\"] = parser.match.date.strftime(\"%Y-%m-%d\") if hasattr(parser, \"match\") and parser.match and parser.match.date else None\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    event_types = {\n",
    "        \"player_death\": [\"X\", \"Y\", \"attacker\", \"victim\", \"weapon\", \"total_rounds_played\"],\n",
    "        # Add other event types if needed\n",
    "    }\n",
    "\n",
    "    base_csv_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for event, fields in event_types.items():\n",
    "        folder = base_csv_dir / event\n",
    "        folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            result = parser.parse_event(event, player=fields)\n",
    "            df = pd.DataFrame(result) if isinstance(result, list) else result\n",
    "            df['demo_id'] = demo_id\n",
    "            csv_path = folder / f\"{dem_name}_{event}.csv\"\n",
    "            df.to_csv(csv_path, index=False)\n",
    "            print(f\"âœ… Exported {event} events to {csv_path} (rows={len(df)})\")\n",
    "\n",
    "            if event == \"round_start\":\n",
    "                meta[\"rounds_started\"] = len(df)\n",
    "                meta[\"total_rounds\"] = len(df)\n",
    "            elif event == \"round_end\":\n",
    "                meta[\"rounds_ended\"] = len(df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš  Failed to export {event} events: {e}\")\n",
    "\n",
    "    all_props = [\n",
    "        \"X\", \"Y\", \"Z\", \"health\", \"armor_value\", \"has_helmet\",\n",
    "        \"is_alive\", \"life_state\", \"active_weapon\", \"active_weapon_name\",\n",
    "        \"inventory\", \"total_cash_spent\", \"team_num\", \"is_bomb_dropped\", \"is_bomb_planted\"]\n",
    "\n",
    "    try:\n",
    "        tick_folder = base_csv_dir / \"ticks\"\n",
    "        tick_folder.mkdir(parents=True, exist_ok=True)\n",
    "        ticks_df = parser.parse_ticks([\n",
    "            \"X\",\"Y\",\"Z\",\"health\",\"is_alive\",\n",
    "            \"active_weapon\", \"active_weapon_name\",\n",
    "            \"inventory\", \"total_cash_spent\",\"team_num\"])\n",
    "        ticks_df = add_next_tick_position(ticks_df)\n",
    "        ticks_df['demo_id'] = demo_id\n",
    "        tick_csv_path = tick_folder / f\"{dem_name}_ticks.csv\"\n",
    "        ticks_df.to_csv(tick_csv_path, index=False)\n",
    "        print(f\"âœ… Exported ticks to {tick_csv_path} (rows={len(ticks_df)})\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš  Failed to export ticks: {e}\")\n",
    "\n",
    "def parse_demo_with_timeout(dem_path: pathlib.Path, base_csv_dir: pathlib.Path, master_csv_path: pathlib.Path, match_name: str, timeout_sec=300):\n",
    "    \"\"\"\n",
    "    Parses a demo with a timeout. If parsing exceeds `timeout_sec` seconds, skip and warn.\n",
    "    \"\"\"\n",
    "    \n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=1) as executor:\n",
    "        future = executor.submit(parse_demo, dem_path, base_csv_dir, master_csv_path, match_name)\n",
    "        try:\n",
    "            future.result(timeout=timeout_sec)\n",
    "        except concurrent.futures.TimeoutError:\n",
    "            print(f\"âš  Parsing of {dem_path} took longer than {timeout_sec} seconds and was skipped.\")\n",
    "            # Forcefully terminate the process\n",
    "            executor.shutdown(wait=False, cancel_futures=True)\n",
    "\n",
    "def parse_demos(structured_root):\n",
    "    base_dir = pathlib.Path(\"hltv_demos\")\n",
    "    matches_csv_path = \"hltv_matches.csv\"\n",
    "\n",
    "    df = pd.read_csv(matches_csv_path)\n",
    "\n",
    "    for idx, info in df.iterrows():\n",
    "        dem_path_str = info.get('dem_path', \"\")\n",
    "        \n",
    "        # Check dem_path_str is a valid, non-empty string\n",
    "        if not isinstance(dem_path_str, str) or dem_path_str.strip() == \"\":\n",
    "            print(f\"Skipping demo at row {idx} because dem_path is empty or invalid\")\n",
    "            continue\n",
    "            \n",
    "        status = str(info.get('parsed_status', \"\")).lower()\n",
    "        if not dem_path_str or status in {\"parsed\", \"skip\"}:\n",
    "            print(f\"Skipping demo at row {idx} due to status: '{status}'\")\n",
    "            continue\n",
    "\n",
    "        dem_path = pathlib.Path(dem_path_str)\n",
    "        match_name = info.get('match_name', \"\")\n",
    "        demo_url = info.get('demo_url','')\n",
    "        demo_id = demo_url.rstrip('/').split('/')[-1]\n",
    "        try:\n",
    "            # Call parse_demo without master_csv_path argument\n",
    "            parse_demo(dem_path, structured_root, match_name, demo_id)\n",
    "            \n",
    "            df.at[idx, 'parsed_status'] = \"parsed\"\n",
    "            df.to_csv(matches_csv_path, index=False)\n",
    "            print(f\"âœ… Parsed demo {dem_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš  Failed to parse demo {dem_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4e1174-50d8-4d79-bf7a-66e30c93f3ea",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26b4faf-19b5-4a68-a9b5-9cb035b151c0",
   "metadata": {},
   "source": [
    "### Coordinate Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64793ee1-a368-4521-81c7-d4f3588556d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_map_data(csv_file='map_bounds-in.csv'):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Group coordinates by map and area name\n",
    "    areas = {}\n",
    "    for (map_name, area_name), group in df.groupby(['map', 'name']):\n",
    "        # Extract x, y coordinates\n",
    "        coords = list(zip(group['x_axis'], group['y_axis']))\n",
    "        \n",
    "        # Only create polygon if we have at least 3 points\n",
    "        if len(coords) >= 3:\n",
    "            areas[area_name] = Polygon(coords)\n",
    "    \n",
    "    return areas\n",
    "\n",
    "def label_coordinate(x, y, areas):\n",
    "    \"\"\"\n",
    "    Given X and Y coordinates, return the label of the area.\n",
    "    If not inside any area, returns the closest area's name.\n",
    "    \"\"\"\n",
    "    point = Point(x, y)\n",
    "    min_dist = float('inf')\n",
    "    closest_area = \"Unknown\"\n",
    "    for area_name, polygon in areas.items():\n",
    "        if polygon.contains(point):\n",
    "            return area_name\n",
    "        # Track closest polygon by centroid distance\n",
    "        d = point.distance(polygon.centroid)\n",
    "        if d < min_dist:\n",
    "            min_dist = d\n",
    "            closest_area = area_name\n",
    "    return closest_area\n",
    "\n",
    "def label_coordinates_batch(coordinates, areas):\n",
    "    \"\"\"\n",
    "    Label multiple coordinates at once.\n",
    "    \n",
    "    Parameters:\n",
    "    coordinates (list): List of (x, y) tuples\n",
    "    areas (dict): Dictionary of area names mapped to Polygon objects\n",
    "    \n",
    "    Returns:\n",
    "    list: List of area names corresponding to each coordinate\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for x, y in coordinates:\n",
    "        label = label_coordinate(x, y, areas)\n",
    "        results.append(label)\n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_areas(areas):\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    ax = plt.gca()\n",
    "    colors = {}\n",
    "\n",
    "    for area_name, polygon in areas.items():\n",
    "        x, y = polygon.exterior.xy\n",
    "        # Assign a random color to each polygon\n",
    "        if area_name not in colors:\n",
    "            colors[area_name] = (random.random(), random.random(), random.random())\n",
    "        ax.fill(x, y, alpha=0.5, fc=colors[area_name], ec='black', linewidth=1, label=area_name)\n",
    "\n",
    "        # Label the polygon at its centroid\n",
    "        centroid = polygon.centroid\n",
    "        plt.text(centroid.x, centroid.y, area_name, fontsize=10, ha='center', va='center')\n",
    "\n",
    "    plt.xlabel('X Coordinate')\n",
    "    plt.ylabel('Y Coordinate')\n",
    "    plt.title('Map Areas Visualization')\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n",
    "    plt.axis('equal')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d1de89f-09f2-473a-a2a8-0b747bd8e398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_teammates_alive(df):\n",
    "    \"\"\"\n",
    "    Add teammates_alive column to dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'tick', 'team_num', and 'is_alive' columns\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with added 'teammates_alive' column\n",
    "    \"\"\"\n",
    "    df['is_alive_bool'] = df['is_alive'].astype(bool)\n",
    "    \n",
    "    alive_counts_tm = (\n",
    "        df[df['is_alive_bool']]\n",
    "        .groupby(['tick', 'team_num'])\n",
    "        .size()\n",
    "        .reset_index(name='alive_count_tm')\n",
    "    )\n",
    "    \n",
    "    df = df.merge(alive_counts_tm, how='left', on=['tick', 'team_num'])\n",
    "    df['teammates_alive'] = df['alive_count_tm'].fillna(0).astype(int) - df['is_alive'].astype(int)\n",
    "    df = df.drop(columns=['alive_count_tm'], errors='ignore')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def add_enemies_alive(df):\n",
    "    \"\"\"\n",
    "    Add enemies_alive column to dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'tick', 'team_num', and 'is_alive_bool' columns\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with added 'enemies_alive' column\n",
    "    \"\"\"\n",
    "    alive_counts_all = (\n",
    "        df[df['is_alive_bool']]\n",
    "        .groupby(['tick', 'team_num'])\n",
    "        .size()\n",
    "        .reset_index(name='alive_count_any_team')\n",
    "    )\n",
    "    \n",
    "    total_alive_per_tick = (\n",
    "        alive_counts_all\n",
    "        .groupby('tick')['alive_count_any_team']\n",
    "        .sum()\n",
    "        .reset_index(name='total_alive')\n",
    "    )\n",
    "    \n",
    "    alive_counts_all = alive_counts_all.merge(total_alive_per_tick, on='tick')\n",
    "    alive_counts_all['enemies_alive'] = (\n",
    "        alive_counts_all['total_alive'] - alive_counts_all['alive_count_any_team']\n",
    "    )\n",
    "    \n",
    "    df = df.merge(\n",
    "        alive_counts_all[['tick', 'team_num', 'enemies_alive']],\n",
    "        how='left',\n",
    "        on=['tick', 'team_num']\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def add_teammate_zones(df, label_col='current_zone', max_teammates=4):\n",
    "    \"\"\"\n",
    "    Add teammate zone columns to dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'tick', 'team_num', 'is_alive_bool', and label_col columns\n",
    "        label_col: Name of the column containing zone labels\n",
    "        max_teammates: Maximum number of teammates to track (default 4 for CS:GO)\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with added 'teammate_1_zone', 'teammate_2_zone', etc. columns\n",
    "    \"\"\"\n",
    "    # Aggregate all zones for each team at each tick\n",
    "    teammate_zones = (\n",
    "        df[df['is_alive_bool']]\n",
    "        .groupby(['tick', 'team_num'])[label_col]\n",
    "        .agg(lambda x: list(x))\n",
    "        .reset_index()\n",
    "        .rename(columns={label_col: 'teammate_zones_all'})\n",
    "    )\n",
    "    \n",
    "    df = df.merge(teammate_zones, how='left', on=['tick', 'team_num'])\n",
    "    \n",
    "    # Extract teammate zones (excluding own zone) into separate columns\n",
    "    def extract_teammate_zones(row):\n",
    "        zones_all = row['teammate_zones_all']\n",
    "        \n",
    "        # Check if value is NaN or not a list\n",
    "        if not isinstance(zones_all, list):\n",
    "            zones = []\n",
    "        else:\n",
    "            zones = zones_all.copy()\n",
    "            try:\n",
    "                zones.remove(row[label_col])  # Remove player's own zone\n",
    "            except ValueError:\n",
    "                pass  # Own zone not in list (e.g., player is dead)\n",
    "        \n",
    "        # Pad or truncate to max_teammates\n",
    "        zones = zones[:max_teammates]  # Truncate if more\n",
    "        zones += [''] * (max_teammates - len(zones))  # Pad if fewer\n",
    "        \n",
    "        return pd.Series({f'teammate_{i+1}_zone': zone for i, zone in enumerate(zones)})\n",
    "    \n",
    "    # Apply and concatenate teammate zone columns\n",
    "    teammate_cols = df.apply(extract_teammate_zones, axis=1)\n",
    "    df = pd.concat([df, teammate_cols], axis=1)\n",
    "    \n",
    "    # Clean up temporary column\n",
    "    df = df.drop(columns=['teammate_zones_all'], errors='ignore')\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43393f96-4755-4839-82cb-9d7300b39b09",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcac976-07d1-444d-a318-946d9b6b7569",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Scraping Master List of Demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca86327-eb1d-4b36-bf9f-919cacf1fbbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "HLTV_BASE = \"https://www.hltv.org\"\n",
    "Filters = \"?startDate=2024-11-15&endDate=2025-11-15&stars=2&map=de_mirage\"\n",
    "RESULTS_URL = f\"{HLTV_BASE}/results{Filters}\"\n",
    "REQUEST_DELAY_SEC = 5\n",
    "print(RESULTS_URL)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Setup\n",
    "    download_dir = pathlib.Path(\"./downloads\")\n",
    "    download_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    driver = setup_undetected_chrome(download_dir)\n",
    "    \n",
    "    try:\n",
    "        # You may want to pass a less filtered results URL to see if this works!\n",
    "        match_urls = find_match_urls_from_results(driver, max_matches=100, delay=5)\n",
    "        if not match_urls:\n",
    "            print(\"No matches found with the given parameters and filters.\")\n",
    "        else:\n",
    "            matches = scrape_multiple_matches(driver, match_urls)\n",
    "            if not matches:\n",
    "                print(\"No match data extracted (possibly due to page change or filter).\")\n",
    "            else:\n",
    "                df = save_match_data_to_csv(matches, \"hltv_matches.csv\")\n",
    "                print(\"\\n=== Summary ===\")\n",
    "                print(df)\n",
    "    finally:\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa5a2a6-9f8d-4eb9-8854-9c63493cf183",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Download, Extract, Parse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973ae654-2b64-44b1-930e-a732c090f40b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf37d04-d4e7-4289-a684-cdbc682aa12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract(max_downloads=None, name_filter: str = None):\n",
    "    base_dir = pathlib.Path(\"hltv_demos\")\n",
    "    download_root = base_dir / \"downloads\"\n",
    "    extract_root = base_dir / \"extracted\"\n",
    "    download_root.mkdir(parents=True, exist_ok=True)\n",
    "    extract_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    REQUEST_DELAY_SEC = 5\n",
    "\n",
    "    df = pd.read_csv(\"hltv_matches.csv\")\n",
    "    filtered = df[df[\"demo_url\"].notnull()]\n",
    "    print(f\"Found {len(filtered)} demo links in CSV.\")\n",
    "\n",
    "    # Add or initialize columns if missing\n",
    "    if 'dem_path' not in df.columns:\n",
    "        df['dem_path'] = \"\"\n",
    "    if 'match_name' not in df.columns:\n",
    "        df['match_name'] = \"\"\n",
    "    if 'parsed_status' not in df.columns:\n",
    "        df['parsed_status'] = \"\"\n",
    "\n",
    "    # Skip rows already parsed or marked to skip\n",
    "    filtered = filtered[(df['parsed_status'] != \"parsed\") & (df['parsed_status'] != \"skip\")]\n",
    "\n",
    "    # Limit number of downloads if specified\n",
    "    if max_downloads is not None:\n",
    "        filtered = filtered.iloc[:max_downloads]\n",
    "\n",
    "    for idx, row in filtered.iterrows():\n",
    "        match_url = row[\"match_url\"]\n",
    "        demo_url = row[\"demo_url\"]\n",
    "        print(f\"\\n === [{idx+1}/{len(filtered)}] {match_url} ===\")\n",
    "        print(f\"Demo URL: {demo_url}\")\n",
    "        time.sleep(10)\n",
    "\n",
    "        try:\n",
    "            demo_number = get_demo_number_from_url(demo_url)\n",
    "            demo_download_dir = download_root / demo_number\n",
    "            demo_download_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            driver = setup_undetected_chrome(download_dir=demo_download_dir, headless=False)\n",
    "            archive_path = download_demo_with_selenium(driver, demo_url, demo_download_dir)\n",
    "            match_name = archive_path.stem\n",
    "\n",
    "            while any(demo_download_dir.glob(\"*.crdownload\")):\n",
    "                print(\"Waiting for partial downloads to finish...\")\n",
    "                time.sleep(30)\n",
    "\n",
    "            before_extraction = set(extract_root.glob(\"*.dem\"))\n",
    "            dem_path = extract_dem_from_file(archive_path, extract_root, name_filter)\n",
    "            after_extraction = set(extract_root.glob(\"*.dem\"))\n",
    "            new_dem_files = after_extraction - before_extraction\n",
    "\n",
    "            for new_dem in new_dem_files:\n",
    "                existing_dem_path = df.at[idx, 'dem_path']\n",
    "                if pd.isna(existing_dem_path) or existing_dem_path == \"\":\n",
    "                    df.at[idx, 'dem_path'] = str(new_dem)\n",
    "                else:\n",
    "                    print(f\"âš  dem_path already exists for row {idx}, skipping update.\")\n",
    "\n",
    "                existing_match_name = df.at[idx, 'match_name']\n",
    "                if pd.isna(existing_match_name) or existing_match_name == \"\":\n",
    "                    df.at[idx, 'match_name'] = match_name\n",
    "                else:\n",
    "                    print(f\"âš  match_name already exists for row {idx}, skipping update.\")\n",
    "            \n",
    "            # Mark this row as 'downloaded' and ready for parsing or set your own logic here\n",
    "            df.at[idx, 'Status'] = \"downloaded\"\n",
    "\n",
    "            driver.quit()\n",
    "        except Exception as e:\n",
    "            print(f\"âš  Skipping this demo due to error: {e}\")\n",
    "            df.at[idx, 'parsed_status'] = \"Skip\"\n",
    "\n",
    "        time.sleep(REQUEST_DELAY_SEC)\n",
    "\n",
    "    # Overwrite the original CSV\n",
    "    df.to_csv(\"hltv_matches.csv\", index=False)\n",
    "    print(\"Updated hltv_matches.csv with enriched demo info and parsing status.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450509a8-a4a3-4ced-a725-9fb03deb1a09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_dir = pathlib.Path(\"hltv_demos\")\n",
    "structured_root = base_dir / \"structured\"\n",
    "structured_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "demo_infos = download_and_extract(max_downloads=20, name_filter = \"mirage\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7863e5-a47f-4917-af84-21c6b8236a81",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060e3929-9993-47f2-8bb1-d96195a9a1c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_dir = pathlib.Path(\"hltv_demos\")\n",
    "structured_root = base_dir / \"structured\"\n",
    "structured_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load enriched hltv_matches.csv containing all demo metadata and parsing status\n",
    "matches_csv_path = \"hltv_matches.csv\"\n",
    "df_matches = pd.read_csv(matches_csv_path)\n",
    "\n",
    "# Convert 'dem_path' strings to pathlib.Path objects for parsing function\n",
    "df_matches['dem_path'] = df_matches['dem_path'].apply(lambda p: pathlib.Path(p) if pd.notna(p) and p != \"\" else None)\n",
    "\n",
    "# Call parse_demos with the DataFrame instead of a separate demo_infos list\n",
    "parse_demos(structured_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ef30c3-6fd2-46d7-ab56-f26691c329c8",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "460c24f7-7fa1-4011-82a3-ab3a4339fce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_all_ticks_in_dir(\n",
    "    ticks_dir,\n",
    "    areas,\n",
    "    x_col=\"X\",\n",
    "    y_col=\"Y\",\n",
    "    label_col=\"current_zone\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Label all tick CSV files in a directory with zone and teammate information.\n",
    "    \n",
    "    Args:\n",
    "        ticks_dir: Path to directory containing tick CSV files\n",
    "        areas: Dict of area_name -> shapely Polygon for zone labeling\n",
    "        x_col: Name of X coordinate column\n",
    "        y_col: Name of Y coordinate column\n",
    "        label_col: Name of the zone label column\n",
    "    \"\"\"\n",
    "    ticks_dir = Path(ticks_dir)\n",
    "    labeled_dir = ticks_dir.parent / \"labeled\"\n",
    "    labeled_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Load and prepare matches tracking CSV\n",
    "    matches_csv_path = \"hltv_matches.csv\"\n",
    "    df_matches = pd.read_csv(matches_csv_path)\n",
    "    \n",
    "    if 'labeled_status' not in df_matches.columns:\n",
    "        df_matches['labeled_status'] = pd.Series(dtype='string')\n",
    "    else:\n",
    "        df_matches['labeled_status'] = df_matches['labeled_status'].astype('string')\n",
    "    \n",
    "    # Process each tick file\n",
    "    files = list(ticks_dir.glob(\"*.csv\"))\n",
    "    for file in files:\n",
    "        dem_name_from_tick = file.stem.replace('_ticks', '')\n",
    "        \n",
    "        # Find matching row in hltv_matches.csv\n",
    "        match_rows = df_matches[\n",
    "            df_matches['dem_path'].apply(\n",
    "                lambda p: pathlib.Path(p).stem if pd.notna(p) else ''\n",
    "            ) == dem_name_from_tick\n",
    "        ]\n",
    "        \n",
    "        # Check if already labeled or should skip\n",
    "        if not match_rows.empty:\n",
    "            val = match_rows.iloc[0].get('labeled_status')\n",
    "            status = '' if pd.isna(val) else str(val).lower()\n",
    "            \n",
    "            if status in {\"labeled\", \"skip\"}:\n",
    "                print(f\"Skipping labeling for {file.name} due to status: '{status}'\")\n",
    "                continue\n",
    "                \n",
    "            row_index = match_rows.index[0]\n",
    "        else:\n",
    "            row_index = None\n",
    "        \n",
    "        # Process the file\n",
    "        print(f\"Processing {file.name} ...\")\n",
    "        df = pd.read_csv(file, low_memory=False)\n",
    "        \n",
    "        # Filter to every 16th tick and remove invalid coordinates\n",
    "        df = df[df[\"tick\"] % 16 == 0]\n",
    "        df = df.dropna(subset=[x_col, y_col])\n",
    "        \n",
    "        # Label zones using coordinate labeling function\n",
    "        df[label_col] = df.apply(\n",
    "            lambda row: label_coordinate(row[x_col], row[y_col], areas), \n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Add all features\n",
    "        df = add_teammates_alive(df)\n",
    "        df = add_enemies_alive(df)\n",
    "        df = add_teammate_zones(df, label_col=label_col)\n",
    "        \n",
    "        # Clean up temporary columns\n",
    "        df = df.drop(columns=['is_alive_bool'], errors='ignore')\n",
    "        \n",
    "        # Save labeled file\n",
    "        output_path = labeled_dir / file.name\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"Saved labeled CSV to {output_path}\")\n",
    "        \n",
    "        # Update status in tracking CSV\n",
    "        if row_index is not None:\n",
    "            df_matches.at[row_index, 'labeled_status'] = 'labeled'\n",
    "        else:\n",
    "            print(f\"Warning: No matching row found in hltv_matches.csv for {file.name}\")\n",
    "    \n",
    "    # Save updated tracking CSV\n",
    "    df_matches.to_csv(matches_csv_path, index=False)\n",
    "    print(f\"Processing complete. Updated {matches_csv_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f3ae5a3-9de2-48f8-be85-2692d90c27e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping labeling for astralis-vs-falcons-m2-mirage_ticks.csv due to status: 'labeled'\n",
      "Skipping labeling for astralis-vs-natus-vincere-m2-mirage_ticks.csv due to status: 'labeled'\n",
      "Skipping labeling for astralis-vs-vitality-m3-mirage_ticks.csv due to status: 'labeled'\n",
      "Skipping labeling for aurora-vs-heroic-m1-mirage_ticks.csv due to status: 'labeled'\n",
      "Skipping labeling for aurora-vs-legacy-m2-mirage_ticks.csv due to status: 'labeled'\n",
      "Skipping labeling for aurora-vs-the-mongolz-m2-mirage_ticks.csv due to status: 'labeled'\n",
      "Skipping labeling for falcons-vs-furia-m2-mirage_ticks.csv due to status: 'labeled'\n",
      "Skipping labeling for furia-vs-natus-vincere-m1-mirage_ticks.csv due to status: 'labeled'\n",
      "Skipping labeling for furia-vs-pain-m3-mirage_ticks.csv due to status: 'labeled'\n",
      "Skipping labeling for furia-vs-the-mongolz-m1-mirage_ticks.csv due to status: 'labeled'\n",
      "Skipping labeling for legacy-vs-liquid-m1-mirage_ticks.csv due to status: 'labeled'\n",
      "Skipping labeling for mouz-vs-falcons-m3-mirage_ticks.csv due to status: 'labeled'\n",
      "Skipping labeling for natus-vincere-vs-aurora-m2-mirage_ticks.csv due to status: 'labeled'\n",
      "Skipping labeling for natus-vincere-vs-the-mongolz-m1-mirage_ticks.csv due to status: 'labeled'\n",
      "Skipping labeling for spirit-vs-the-mongolz-m3-mirage_ticks.csv due to status: 'labeled'\n",
      "Processing spirit-vs-vitality-m3-mirage_ticks.csv ...\n",
      "Saved labeled CSV to hltv_demos\\structured\\labeled\\spirit-vs-vitality-m3-mirage_ticks.csv\n",
      "Skipping labeling for vitality-vs-g2-m2-mirage_ticks.csv due to status: 'labeled'\n",
      "Processing complete. Updated hltv_matches.csv\n"
     ]
    }
   ],
   "source": [
    "areas = load_map_data('map_bounds(in).csv')\n",
    "labeled = label_all_ticks_in_dir(\"hltv_demos/structured/ticks\", areas) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784f3961-d096-46ae-ac8b-49ef33853ec0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475e2859-d650-476d-99b1-e5dcad0929ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    base_dir = pathlib.Path(\"hltv_demos\")\n",
    "    download_root = base_dir / \"downloads\"\n",
    "    extract_root = base_dir / \"extracted\"\n",
    "    structured_root = base_dir / \"structured\"\n",
    "    download_root.mkdir(parents=True, exist_ok=True)\n",
    "    extract_root.mkdir(parents=True, exist_ok=True)\n",
    "    structured_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    master_csv_path = structured_root / \"master.csv\"  # path to master file\n",
    "\n",
    "    driver = setup_undetected_chrome(download_dir=download_root)\n",
    "\n",
    "    try:\n",
    "        match_urls = find_match_urls_from_results(driver, max_matches=20, delay=5)\n",
    "        if not match_urls:\n",
    "            raise RuntimeError(\"No match links found on results page.\")\n",
    "\n",
    "        # Step 1: Download, extract, and immediately parse each demo\n",
    "        for idx, match_url in enumerate(match_urls, start=1):\n",
    "            print(f\"\\n === [{idx}/{len(match_urls)}] {match_url} ===\")\n",
    "            try:\n",
    "                demo_url = find_demo_download_url(driver, match_url)\n",
    "                print(f\"Demo URL: {demo_url}\")\n",
    "\n",
    "                archive_path = download_demo_with_selenium(driver, demo_url, download_root)\n",
    "                \n",
    "                # Extract match name from archive filename (e.g., .rar file stem)\n",
    "                match_name = archive_path.stem\n",
    "                \n",
    "                # Track .dem files before extraction\n",
    "                before_extraction = set(extract_root.glob(\"*.dem\"))\n",
    "                \n",
    "                # Extract archive\n",
    "                dem_path = extract_dem_from_file(archive_path, extract_root)\n",
    "                \n",
    "                # Find newly extracted .dem files\n",
    "                after_extraction = set(extract_root.glob(\"*.dem\"))\n",
    "                new_dem_files = after_extraction - before_extraction\n",
    "                \n",
    "                # Parse each newly extracted demo immediately with match name\n",
    "                for new_dem in new_dem_files:\n",
    "                    try:\n",
    "                        parse_demo(new_dem, structured_root, master_csv_path, match_name)\n",
    "                    except Exception as e:\n",
    "                        print(f\"âš  Failed to parse demo {new_dem}: {e}\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"âš  Skipping this match due to error: {e}\")\n",
    "\n",
    "            time.sleep(REQUEST_DELAY_SEC)\n",
    "\n",
    "    finally:\n",
    "        print(\"Closing browser.\")\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e69614-8e7d-4a7b-920b-e4624cf4e399",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac12c60d-1f64-40c7-a1a4-e824abf8de19",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Coordinate Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4096485-3e9e-45dc-9908-9c02ebbeda64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the map data\n",
    "    areas = load_map_data('map_bounds(in).csv')\n",
    "    \n",
    "    print(f\"Loaded {len(areas)} areas from the map\\n\")\n",
    "    \n",
    "    # Test with some example coordinates\n",
    "    test_coordinates = [\n",
    "        (-1, -600),   # Should be in t_spawn area\n",
    "        (700, 2500),    # Should be in bombsite_A area\n",
    "        (-2000, 2000),  # Should be in bombsite_B area\n",
    "        (0, 0),         # Unknown area\n",
    "        (600, 1000),    # Should be in long_A area\n",
    "    ]\n",
    "    \n",
    "    print(\"Testing coordinates:\")\n",
    "    print(\"-\" * 50)\n",
    "    for x, y in test_coordinates:\n",
    "        label = label_coordinate(x, y, areas)\n",
    "        print(f\"Coordinate ({x:6}, {y:6}) -> {label}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"\\nBatch labeling example:\")\n",
    "    labels = label_coordinates_batch(test_coordinates, areas)\n",
    "    for (x, y), label in zip(test_coordinates, labels):\n",
    "        print(f\"({x:6}, {y:6}): {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd07f00d-dfc7-45b3-a758-476f70e2817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "areas = load_map_data('map_bounds(in).csv')\n",
    "plot_areas(areas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
